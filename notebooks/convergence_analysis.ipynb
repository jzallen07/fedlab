{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FedForge Convergence and Telemetry Analysis (I06)\n",
    "\n",
    "This notebook reads generated simulation/profile artifacts and monitor telemetry exports,\n",
    "then produces summary tables and analysis outputs under `artifacts/analysis/i06`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import sqlite3\n",
    "from datetime import UTC, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def _repo_root() -> Path:\n",
    "    cwd = Path.cwd()\n",
    "    if (cwd / \"artifacts\").exists() and (cwd / \"src\").exists():\n",
    "        return cwd\n",
    "    if (cwd.parent / \"artifacts\").exists() and (cwd.parent / \"src\").exists():\n",
    "        return cwd.parent\n",
    "    raise RuntimeError(\"Could not resolve repository root from current working directory\")\n",
    "\n",
    "\n",
    "REPO_ROOT = _repo_root()\n",
    "ANALYSIS_DIR = REPO_ROOT / \"artifacts\" / \"analysis\" / \"i06\"\n",
    "ANALYSIS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PROFILE_REFERENCE_PATHS = sorted((REPO_ROOT / \"artifacts\" / \"profiles\").glob(\"*/reference_metrics.json\"))\n",
    "SERVER_ROUND_METRICS_PATH = REPO_ROOT / \"artifacts\" / \"server\" / \"round_metrics.jsonl\"\n",
    "MONITOR_DB_PATH = REPO_ROOT / \"artifacts\" / \"monitor\" / \"monitor.db\"\n",
    "\n",
    "if not PROFILE_REFERENCE_PATHS:\n",
    "    raise RuntimeError(\"No profile reference metrics found under artifacts/profiles\")\n",
    "if not SERVER_ROUND_METRICS_PATH.exists():\n",
    "    raise RuntimeError(f\"Missing server round metrics file: {SERVER_ROUND_METRICS_PATH}\")\n",
    "if not MONITOR_DB_PATH.exists():\n",
    "    raise RuntimeError(f\"Missing monitor SQLite database: {MONITOR_DB_PATH}\")\n",
    "\n",
    "print(f\"repo_root={REPO_ROOT}\")\n",
    "print(f\"analysis_dir={ANALYSIS_DIR}\")\n",
    "print(f\"profile_reference_files={len(PROFILE_REFERENCE_PATHS)}\")\n",
    "print(f\"server_round_metrics={SERVER_ROUND_METRICS_PATH}\")\n",
    "print(f\"monitor_db={MONITOR_DB_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_reports: list[dict[str, object]] = []\n",
    "for path in PROFILE_REFERENCE_PATHS:\n",
    "    payload = json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    payload[\"reference_path\"] = str(path)\n",
    "    profile_reports.append(payload)\n",
    "\n",
    "profile_overview_rows: list[dict[str, object]] = []\n",
    "for report in profile_reports:\n",
    "    profile_overview_rows.append(\n",
    "        {\n",
    "            \"profile_id\": report[\"profile_id\"],\n",
    "            \"dataset_id\": report[\"dataset_id\"],\n",
    "            \"resolved_device\": report[\"resolved_device\"],\n",
    "            \"baseline_rounds\": report[\"baseline_rounds\"],\n",
    "            \"baseline_clients\": report[\"baseline_clients\"],\n",
    "            \"repeat_runs\": report[\"repeat_runs\"],\n",
    "            \"repeatable\": report[\"repeatable\"],\n",
    "            \"max_loss_delta_vs_run_1\": report[\"max_loss_delta_vs_run_1\"],\n",
    "            \"reference_path\": report[\"reference_path\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "profile_overview_df = pd.DataFrame(profile_overview_rows).sort_values(\"profile_id\")\n",
    "print(\"\\n[Profile Overview]\")\n",
    "print(profile_overview_df.to_string(index=False))\n",
    "profile_overview_df.to_csv(ANALYSIS_DIR / \"profile_overview.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergence_rows: list[dict[str, object]] = []\n",
    "\n",
    "for report in profile_reports:\n",
    "    for run in report[\"runs\"]:\n",
    "        run_index = int(run[\"run_index\"])\n",
    "        losses = run[\"aggregate_eval_loss_by_round\"]\n",
    "        for round_idx, loss in enumerate(losses, start=1):\n",
    "            convergence_rows.append(\n",
    "                {\n",
    "                    \"profile_id\": report[\"profile_id\"],\n",
    "                    \"dataset_id\": report[\"dataset_id\"],\n",
    "                    \"run_index\": run_index,\n",
    "                    \"round\": round_idx,\n",
    "                    \"aggregate_eval_loss\": float(loss),\n",
    "                }\n",
    "            )\n",
    "\n",
    "convergence_df = pd.DataFrame(convergence_rows).sort_values([\"profile_id\", \"run_index\", \"round\"])\n",
    "print(\"\\n[Convergence Points]\")\n",
    "print(convergence_df.to_string(index=False))\n",
    "convergence_df.to_csv(ANALYSIS_DIR / \"convergence_points.csv\", index=False)\n",
    "\n",
    "final_loss_df = (\n",
    "    convergence_df.sort_values(\"round\")\n",
    "    .groupby([\"profile_id\", \"dataset_id\", \"run_index\"], as_index=False)\n",
    "    .tail(1)\n",
    "    .rename(columns={\"aggregate_eval_loss\": \"final_aggregate_eval_loss\"})\n",
    "    [[\"profile_id\", \"dataset_id\", \"run_index\", \"round\", \"final_aggregate_eval_loss\"]]\n",
    ")\n",
    "\n",
    "print(\"\\n[Final Loss per Run]\")\n",
    "print(final_loss_df.to_string(index=False))\n",
    "final_loss_df.to_csv(ANALYSIS_DIR / \"final_loss_by_run.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_rows: list[dict[str, object]] = []\n",
    "\n",
    "for report in profile_reports:\n",
    "    run_one = next((run for run in report[\"runs\"] if int(run[\"run_index\"]) == 1), None)\n",
    "    if run_one is None:\n",
    "        continue\n",
    "\n",
    "    summary_path = Path(str(run_one[\"summary_path\"]))\n",
    "    if not summary_path.is_absolute():\n",
    "        summary_path = REPO_ROOT / summary_path\n",
    "    summary_payload = json.loads(summary_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "    for round_payload in summary_payload[\"round_summaries\"]:\n",
    "        round_number = int(round_payload[\"round\"])\n",
    "        train_metrics = round_payload[\"client_train_metrics\"]\n",
    "        eval_metrics = round_payload[\"client_eval_metrics\"]\n",
    "        for client_id, client_eval in eval_metrics.items():\n",
    "            client_train = train_metrics.get(client_id, {})\n",
    "            client_rows.append(\n",
    "                {\n",
    "                    \"profile_id\": report[\"profile_id\"],\n",
    "                    \"dataset_id\": report[\"dataset_id\"],\n",
    "                    \"round\": round_number,\n",
    "                    \"client_id\": client_id,\n",
    "                    \"eval_loss\": float(client_eval.get(\"eval_loss\", 0.0)),\n",
    "                    \"eval_accuracy\": float(client_eval.get(\"eval_accuracy\", 0.0)),\n",
    "                    \"train_loss\": float(client_train.get(\"train_loss\", 0.0)),\n",
    "                    \"train_runtime\": float(client_train.get(\"train_runtime\", 0.0)),\n",
    "                }\n",
    "            )\n",
    "\n",
    "client_df = pd.DataFrame(client_rows).sort_values([\"profile_id\", \"round\", \"client_id\"])\n",
    "print(\"\\n[Per-Client Metrics from run_01 summaries]\")\n",
    "print(client_df.to_string(index=False))\n",
    "client_df.to_csv(ANALYSIS_DIR / \"client_metrics.csv\", index=False)\n",
    "\n",
    "client_agg_df = (\n",
    "    client_df.groupby([\"profile_id\", \"dataset_id\"], as_index=False)\n",
    "    .agg(\n",
    "        mean_eval_loss=(\"eval_loss\", \"mean\"),\n",
    "        mean_eval_accuracy=(\"eval_accuracy\", \"mean\"),\n",
    "        mean_train_loss=(\"train_loss\", \"mean\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\n[Per-Profile Client Aggregates]\")\n",
    "print(client_agg_df.to_string(index=False))\n",
    "client_agg_df.to_csv(ANALYSIS_DIR / \"client_metrics_aggregated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_round_rows: list[dict[str, object]] = []\n",
    "for line in SERVER_ROUND_METRICS_PATH.read_text(encoding=\"utf-8\").splitlines():\n",
    "    if not line.strip():\n",
    "        continue\n",
    "    payload = json.loads(line)\n",
    "    metrics = payload.get(\"aggregated_metrics\", {})\n",
    "    server_round_rows.append(\n",
    "        {\n",
    "            \"round\": int(payload.get(\"round\", 0)),\n",
    "            \"num_clients\": int(payload.get(\"num_clients\", 0)),\n",
    "            \"num_examples\": int(payload.get(\"num_examples\", 0)),\n",
    "            \"eval_loss\": float(metrics.get(\"eval_loss\", 0.0)),\n",
    "            \"eval_accuracy\": float(metrics.get(\"eval_accuracy\", 0.0)),\n",
    "            \"train_loss\": float(metrics.get(\"train_loss\", 0.0)),\n",
    "            \"train_runtime\": float(metrics.get(\"train_runtime\", 0.0)),\n",
    "            \"ts\": payload.get(\"ts\", \"\"),\n",
    "        }\n",
    "    )\n",
    "\n",
    "server_round_df = pd.DataFrame(server_round_rows).sort_values(\"round\")\n",
    "print(\"\\n[Server Round Aggregates]\")\n",
    "print(server_round_df.to_string(index=False))\n",
    "server_round_df.to_csv(ANALYSIS_DIR / \"server_round_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sqlite3.connect(MONITOR_DB_PATH) as connection:\n",
    "    event_counts_df = pd.read_sql_query(\n",
    "        \"\"\"\n",
    "        SELECT run_id, event_type, COUNT(*) AS count\n",
    "        FROM events\n",
    "        GROUP BY run_id, event_type\n",
    "        ORDER BY run_id, event_type\n",
    "        \"\"\",\n",
    "        connection,\n",
    "    )\n",
    "    latency_df = pd.read_sql_query(\n",
    "        \"\"\"\n",
    "        SELECT role, event_type, AVG(latency_ms) AS avg_latency_ms, COUNT(latency_ms) AS samples\n",
    "        FROM events\n",
    "        WHERE latency_ms IS NOT NULL\n",
    "        GROUP BY role, event_type\n",
    "        ORDER BY role, event_type\n",
    "        \"\"\",\n",
    "        connection,\n",
    "    )\n",
    "    run_state_df = pd.read_sql_query(\n",
    "        \"SELECT run_id, state, updated_at FROM run_state ORDER BY run_id\",\n",
    "        connection,\n",
    "    )\n",
    "\n",
    "print(\"\\n[Monitor Event Counts]\")\n",
    "print(event_counts_df.head(30).to_string(index=False))\n",
    "print(\"\\n[Monitor Event Latencies]\")\n",
    "print(latency_df.head(30).to_string(index=False))\n",
    "print(\"\\n[Monitor Run States]\")\n",
    "print(run_state_df.to_string(index=False))\n",
    "\n",
    "event_counts_df.to_csv(ANALYSIS_DIR / \"monitor_event_counts.csv\", index=False)\n",
    "latency_df.to_csv(ANALYSIS_DIR / \"monitor_event_latencies.csv\", index=False)\n",
    "run_state_df.to_csv(ANALYSIS_DIR / \"monitor_run_states.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_summary = {\n",
    "    \"generated_at_utc\": datetime.now(UTC).isoformat(),\n",
    "    \"profile_count\": int(len(profile_overview_df)),\n",
    "    \"datasets\": sorted(profile_overview_df[\"dataset_id\"].unique().tolist()),\n",
    "    \"all_profiles_repeatable\": bool(profile_overview_df[\"repeatable\"].all()),\n",
    "    \"max_profile_loss_delta\": float(profile_overview_df[\"max_loss_delta_vs_run_1\"].max()),\n",
    "    \"server_round_count\": int(len(server_round_df)),\n",
    "    \"monitor_total_events\": int(event_counts_df[\"count\"].sum()) if not event_counts_df.empty else 0,\n",
    "    \"monitor_run_ids\": sorted(event_counts_df[\"run_id\"].unique().tolist()) if not event_counts_df.empty else [],\n",
    "}\n",
    "\n",
    "summary_path = ANALYSIS_DIR / \"analysis_summary.json\"\n",
    "summary_path.write_text(json.dumps(analysis_summary, indent=2, sort_keys=True), encoding=\"utf-8\")\n",
    "\n",
    "print(\"\\n[Analysis Summary]\")\n",
    "print(json.dumps(analysis_summary, indent=2, sort_keys=True))\n",
    "print(f\"summary_path={summary_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
